{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGURFCcdJfcD"
      },
      "source": [
        "# SWS3009 Lab 6 Statistical Methods\n",
        "\n",
        "\n",
        "|Name      |\n",
        "|:----------|\n",
        "| YANG SIMIN |\n",
        "| PAN DEYU |\n",
        "\n",
        "In this lab you will also be do some experiments to familiarize yourself with the linear regression, Naive Bayes and Support Vector Machine library in SciKit Learn.\n",
        "\n",
        "Please work together as a team of 2 to complete this lab. You will need to submit ONE copy of this notebook per team, but please fill in the names of both team members above. This lab is worth 3 marks:\n",
        "\n",
        "**0 marks**: No submission, empty submission or non-English submission.\n",
        "\n",
        "**1 mark** : Poor submission.\n",
        "\n",
        "**2 marks**: Acceptable submission.\n",
        "\n",
        "**3 marks**: Good submission.\n",
        "\n",
        "## SUBMISSION INSTRUCTIONS\n",
        "\n",
        "Please submit this completed Jupyter Notebook (SWS3009Lab6.ipynb) to the Canvas by **11.59 PM** on **TUESDAY 11 JULY 2023**. All submissions must be in English. Submissions that are not in English will not be marked.\n",
        "\n",
        "Let's now begin using statistical techniques in SciKit Learn.\n",
        "\n",
        "## 1. SciKit Learn Hands-on\n",
        "\n",
        "We will now run some experiments to familiarize you with the statistical learning tools in SciKit Learn.\n",
        "\n",
        "### 1.1 Linear Regression\n",
        "\n",
        "Let's begin by playing around with the linear regression we did for the Boston Housing Dataset during the lecture.\n",
        "\n",
        "#### 1.1.1 Finding Better Correlations\n",
        "\n",
        "In the lecture we looked at correlating housing prices and poverty levels.  Using the code cell below:\n",
        "\n",
        "    1. Recreate the regression example from the lecture.\n",
        "    2. Add code to find the correlation between housing prices and the other independent variables in the dataset.\n",
        "    3. As before save 33% of the data for testing.\n",
        "    4. Create a new simple (single independent variable) regression model with the independent variable with the highest dataset. If poverty levels is the highest, then choose the next highest.\n",
        "    5. Compute and print the MSE for training data and testing data, and answer the questions after the code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM8rLEv-JfcE",
        "outputId": "201c2dfc-0b21-489e-8129-943fd28c9500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between housing prices and independent variables:\n",
            "CRIM      -0.388305\n",
            "ZN         0.360445\n",
            "INDUS     -0.483725\n",
            "CHAS       0.175260\n",
            "NOX       -0.427321\n",
            "RM         0.695360\n",
            "AGE       -0.376955\n",
            "DIS        0.249929\n",
            "RAD       -0.381626\n",
            "TAX       -0.468536\n",
            "PTRATIO   -0.507787\n",
            "B          0.333461\n",
            "LSTAT     -0.737663\n",
            "dtype: float64\n",
            "Independent variable with the highest correlation: LSTAT\n",
            "MSE for training data: 6.1922\n",
            "MSE for testing data: 6.2429\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.1.1 here, and answer the questions\n",
        "    after this code cell.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "bos = pd.DataFrame(data, columns=feature_names)\n",
        "bos['PRICE'] = target\n",
        "\n",
        "# Find correlations between housing prices and other independent variables\n",
        "correlations = bos.drop('PRICE', axis=1).apply(lambda x: bos['PRICE'].corr(x))\n",
        "print(\"Correlation between housing prices and independent variables:\")\n",
        "print(correlations)\n",
        "\n",
        "# Select the independent variable with the highest correlation (excluding PRICE itself)\n",
        "highest_corr_feature = correlations.abs().idxmax()\n",
        "print(\"Independent variable with the highest correlation:\", highest_corr_feature)\n",
        "\n",
        "# Prepare the data for regression\n",
        "X = bos[highest_corr_feature].values.reshape(-1, 1)\n",
        "Y = bos['PRICE'].values.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0)\n",
        "\n",
        "# Create a simple linear regression model\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train, Y_train)\n",
        "\n",
        "# Predict housing prices for training and testing data\n",
        "Y_pred_train = lm.predict(X_train)\n",
        "Y_pred_test = lm.predict(X_test)\n",
        "\n",
        "# Compute and print the MSE for training and testing data\n",
        "train_mse = np.sqrt(metrics.mean_squared_error(Y_train, Y_pred_train))\n",
        "test_mse = np.sqrt(metrics.mean_squared_error(Y_test, Y_pred_test))\n",
        "print(\"MSE for training data: %3.4f\" % train_mse)\n",
        "print(\"MSE for testing data: %3.4f\" % test_mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0EWdQEJfcF"
      },
      "source": [
        "Answer the following questions between the \\*\\* markdowns so that your answers appear in bold.\n",
        "\n",
        "***\n",
        "_Question 1: Which independent variable has the highest correlation? Did it have any effect on your training and test accuracy scores? Why or why not?_\n",
        "---\n",
        "***\n",
        "\n",
        "LSTAT.\n",
        "\n",
        " In general, using a highly correlated variable as an input to the regression model can potentially improve the model's performance.\n",
        "\n",
        " The effect of using the independent variable with the highest correlation on the training and test accuracy scores depends on the dataset and the relationship between the independent variable and the target variable (housing prices).\n",
        "\n",
        "  Correlation alone does not guarantee a strong relationship or predictive power.  Other factors such as linearity, outliers, and the presence of other relevant variables also influence the model's accuracy.  Therefore, the impact on the training and test accuracy scores cannot be determined solely based on the correlation.\n",
        "\n",
        "#### 1.1.2 Creating Multivariate Linear Regressions ####\n",
        "\n",
        "SciKit learn can create linear regression models with multiple independent variables, and in this section we are going to explore how to do this, and whether or not it makes a difference in our Boston Dataset.\n",
        "\n",
        "One way to create a multivariate model is to:\n",
        "\n",
        "    1. Rank the independent variables by correlation, then create a linear model using the independent variable with the highest correlation. Measure the training and testing accuracy.\n",
        "    2. Add in the independent variable with the next highest correlation and create a new linear model.  Measure the training and testing accuracy.\n",
        "    3. Stop when either accuracy score levels off or goes down.\n",
        "\n",
        "Answer the following questions to help you along with creating your multivariate model:\n",
        "\n",
        "***\n",
        "\n",
        "_Question 2: Explain what the following code fragment does. You may refer to NumPy and SciKit Learn documentation_\n",
        "---\n",
        "```\n",
        "bos['PRICE'].values.reshape(-1, 1)\n",
        "```\n",
        "\n",
        "The code fragment bos['PRICE'].values.reshape(-1, 1) reshapes the values of the 'PRICE' column in the bos DataFrame into a 2-dimensional array with a single column.\n",
        "\n",
        "By reshaping the target variable array to have a single column, we adhere to this convention and ensure compatibility with scikit-learn's linear regression models.\n",
        "\n",
        "_Question 3: Consult the NumPy documentation: What does the 'concatenate' function do? In particular what does 'axis=1' do?_\n",
        "---\n",
        "The NumPy concatenate function is used to concatenate or join arrays along a specified axis.\n",
        "\n",
        "np.concatenate(arrays, axis=0): The concatenate function takes two or more arrays as input and returns a single array by concatenating them together along the specified axis.\n",
        "\n",
        "arrays: It is a sequence of arrays that will be concatenated. These arrays should have the same shape along the specified axis, except for the axis along which the concatenation is performed.\n",
        "\n",
        "\n",
        "axis: It specifies the axis along which the arrays will be concatenated. The default value is axis=0, which concatenates the arrays along the first dimension (rows). When axis=1, the arrays are concatenated along the second dimension (columns).\n",
        "\n",
        "\n",
        "To clarify, axis=0 concatenates arrays vertically (stacking them one below the other), while axis=1 concatenates arrays horizontally (joining them side by side).\n",
        "\n",
        "_Question 4: Given your answers to Questions 2 and 3, what does the following code do?_\n",
        "---\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "... Other code here ...\n",
        "\n",
        "X1 = bos['INDUS'].values.reshape(-1, 1)\n",
        "X2 = bos['CRIM'].values.reshape(-1, 1)\n",
        "X = np.concatenate((X1, X2), axis = 1)\n",
        "```\n",
        "\n",
        "\n",
        "The given code takes two independent variables, 'INDUS' and 'CRIM', from the bos DataFrame and creates a new array X by concatenating them along the horizontal axis (axis=1).\n",
        "\n",
        "X1 = bos['INDUS'].values.reshape(-1, 1): Extracts the values of the 'INDUS' column from the bos DataFrame and reshapes it into a 2-dimensional array with a single column using the reshape function. This is done to prepare the data for concatenation.\n",
        "\n",
        "X2 = bos['CRIM'].values.reshape(-1, 1): Extracts the values of the 'CRIM' column from the bos DataFrame and reshapes it into a 2-dimensional array with a single column.\n",
        "\n",
        "X = np.concatenate((X1, X2), axis=1): Concatenates X1 and X2 arrays horizontally along axis=1, resulting in a new array X that contains both the 'INDUS' and 'CRIM' variables side by side.\n",
        "\n",
        "The resulting X array will have a shape of (n_samples, 2), where n_samples is the number of data points in the dataset. Each row represents a data point, and the two columns represent the values of 'INDUS' and 'CRIM' for that data point, respectively.\n",
        "\n",
        "***\n",
        "\n",
        "Use the following code cell to follow the steps above to create models with one, two and three independent variables, printing the training and testing accuracy each time. Note that you have to run _train_test_split_ for each model. Set the _random_state_ parameter in _train_test_split_ to 0 each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BEc_OQXQJfcG",
        "outputId": "ceb185ef-0dd6-4722-8adb-eb5240960a35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with one independent variable (LSTAT):\n",
            "Training Accuracy: 6.1922\n",
            "Testing Accuracy: 6.2429\n",
            "\n",
            "Model with two independent variables (LSTAT and RM):\n",
            "Training Accuracy: 5.4770\n",
            "Testing Accuracy: 5.6286\n",
            "\n",
            "Model with three independent variables (LSTAT and RM and PTRATIO):\n",
            "Training Accuracy: 4.9782\n",
            "Testing Accuracy: 5.6952\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.1.2 here.\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model with one independent variable (highest correlated feature)\n",
        "X1 = bos[highest_corr_feature].values.reshape(-1, 1)\n",
        "Y1 = bos['PRICE'].values.reshape(-1, 1)\n",
        "\n",
        "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.33, random_state=0)\n",
        "\n",
        "lm1 = LinearRegression()\n",
        "lm1.fit(X1_train, Y1_train)\n",
        "\n",
        "Y1_pred_train = lm1.predict(X1_train)\n",
        "Y1_pred_test = lm1.predict(X1_test)\n",
        "\n",
        "train_mse_1 = np.sqrt(metrics.mean_squared_error(Y1_train, Y1_pred_train))\n",
        "test_mse_1 = np.sqrt(metrics.mean_squared_error(Y1_test, Y1_pred_test))\n",
        "\n",
        "print(\"Model with one independent variable ({}):\".format(highest_corr_feature))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_mse_1))\n",
        "print(\"Testing Accuracy: {:.4f}\".format(test_mse_1))\n",
        "print()\n",
        "\n",
        "# Model with two independent variables (highest correlated feature + next highest correlated feature)\n",
        "second_corr_feature = correlations.drop(highest_corr_feature).abs().idxmax()\n",
        "X2 = np.concatenate((X1, bos[second_corr_feature].values.reshape(-1, 1)), axis=1)\n",
        "Y2 = bos['PRICE'].values.reshape(-1, 1)\n",
        "\n",
        "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.33, random_state=0)\n",
        "\n",
        "lm2 = LinearRegression()\n",
        "lm2.fit(X2_train, Y2_train)\n",
        "\n",
        "Y2_pred_train = lm2.predict(X2_train)\n",
        "Y2_pred_test = lm2.predict(X2_test)\n",
        "\n",
        "train_mse_2 = np.sqrt(metrics.mean_squared_error(Y2_train, Y2_pred_train))\n",
        "test_mse_2 = np.sqrt(metrics.mean_squared_error(Y2_test, Y2_pred_test))\n",
        "\n",
        "print(\"Model with two independent variables ({} and {}):\".format(highest_corr_feature, second_corr_feature))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_mse_2))\n",
        "print(\"Testing Accuracy: {:.4f}\".format(test_mse_2))\n",
        "print()\n",
        "\n",
        "# Model with three independent variables (highest correlated feature + next two highest correlated features)\n",
        "third_corr_feature = correlations.drop([highest_corr_feature, second_corr_feature]).abs().idxmax()\n",
        "X3 = np.concatenate((X2, bos[third_corr_feature].values.reshape(-1, 1)), axis=1)\n",
        "Y3 = bos['PRICE'].values.reshape(-1, 1)\n",
        "\n",
        "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size=0.33, random_state=0)\n",
        "\n",
        "lm3 = LinearRegression()\n",
        "lm3.fit(X3_train, Y3_train)\n",
        "\n",
        "Y3_pred_train = lm3.predict(X3_train)\n",
        "Y3_pred_test = lm3.predict(X3_test)\n",
        "\n",
        "train_mse_3 = np.sqrt(metrics.mean_squared_error(Y3_train, Y3_pred_train))\n",
        "test_mse_3 = np.sqrt(metrics.mean_squared_error(Y3_test, Y3_pred_test))\n",
        "\n",
        "print(\"Model with three independent variables ({} and {} and {}):\".format(highest_corr_feature, second_corr_feature, third_corr_feature))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_mse_3))\n",
        "print(\"Testing Accuracy: {:.4f}\".format(test_mse_3))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzEZbIE2JfcG"
      },
      "source": [
        "### 1.2 Creating a Naive Bayes Classifier ###\n",
        "\n",
        "We will now look at how to create a Naive Bayes Classifier, and later on a Support Vector Machine classifier. We will also explore the use of _GridSearchCV_ to optimize the choice of parameters for the SVC.\n",
        "\n",
        "#### 1.2.1 The Irises Dataset ###\n",
        "\n",
        "In this lab we will use the irises dataset to classify four categories of irises (a species of flowers). We will consider four factors:\n",
        "\n",
        "    1. Sepal length in cm\n",
        "    2. Sepal width in cm\n",
        "    3. Petal length in cm\n",
        "    4. Petal width in cm\n",
        "\n",
        "The image below shows what these mean:\n",
        "\n",
        "![iris.png](attachment:image.png)\n",
        "\n",
        "The code cell below loads up the Iris dataset, prints it out, then scales it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZsrgN2aJfcG",
        "outputId": "c696ad15-d98f-41ec-8072-cf3fa8744b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Data:\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "\n",
        "iris_data = load_iris()\n",
        "print(\"Iris Data:\")\n",
        "print(iris_data.data)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_data.data)\n",
        "X = scaler.transform(iris_data.data)\n",
        "Y = iris_data.target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDeocCFnJfcG"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "_Question 5: What does 'StandardScaler' do? What other types of scalers are available? What is the advantage of scaling your inputs?_\n",
        "---\n",
        "The StandardScaler in scikit-learn is a preprocessing class used for standardizing features by removing the mean and scaling to unit variance. It transforms the data such that each feature has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Other types of scalers available in scikit-learn include:\n",
        "\n",
        "MinMaxScaler: Scales features to a given range (e.g., between 0 and 1) by shifting and scaling the data.\n",
        "\n",
        "RobustScaler: Scales features using statistics that are robust to outliers, such as the median and interquartile range.\n",
        "\n",
        "MaxAbsScaler: Scales features to the range [-1, 1] by dividing by the maximum absolute value in each feature.\n",
        "\n",
        "\n",
        "\n",
        "The advantage of scaling your inputs is that it can help improve the performance and effectiveness of many machine learning algorithms. Some reasons for scaling include:\n",
        "\n",
        "Normalization: Scaling ensures that all features have a similar scale, preventing some features from dominating others solely based on their magnitude. It helps to avoid biases in the model.\n",
        "\n",
        "Gradient Descent: Scaling can help speed up the convergence of gradient-based optimization algorithms, such as gradient descent, by making the optimization process more efficient.\n",
        "\n",
        "Distance-Based Algorithms: Many machine learning algorithms rely on calculating distances between data points, such as K-nearest neighbors (KNN) or support vector machines (SVM). Scaling ensures that the distances are computed consistently across all features.\n",
        "\n",
        "Regularization: Some regularization techniques, like L1 and L2 regularization, assume that the features are on the same scale. Scaling helps in properly applying regularization to different features.\n",
        "\n",
        "By scaling the inputs, we can make the data more suitable for modeling, improve algorithm performance, and ensure that the model is not biased towards certain features due to their scale.\n",
        "\n",
        "#### 1.2.2 Creating a Naive Bayes Classifier Model\n",
        "\n",
        "Recall that there are three major types of Naive Bayes classifiers:\n",
        "\n",
        "    1. Gaussian\n",
        "    2. Multinomial\n",
        "    3. Bernoulli\n",
        "    \n",
        "_Question 6: What type of model should we use here? Why?_\n",
        "---\n",
        "For this type of dataset, the appropriate type of Naive Bayes classifier to use is the Gaussian Naive Bayes.\n",
        "\n",
        "The Gaussian Naive Bayes classifier assumes that the features follow a Gaussian (normal) distribution. It calculates the probabilities using the mean and standard deviation of each feature for each class. Since the iris dataset features are continuous variables, Gaussian Naive Bayes is suitable for modeling the relationships between the features and the target classes.\n",
        "\n",
        "Therefore, we should use the Gaussian Naive Bayes classifier in this case to classify the iris dataset based on sepal and petal measurements.\n",
        "\n",
        "\n",
        "Now complete the code in the code cell below, following these specifications:\n",
        "\n",
        "    1. Set aside 20% of the data for testing.\n",
        "    2. Use the appropriate type of Naive Bayes Classifier, adding in whatever import statements you require here.\n",
        "    3. Print out the training and testing accuracies.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNIDPB0_JfcG",
        "outputId": "df5a450a-c51c-4e26-8cd3-c6e4db5c6e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.95\n",
            "Testing Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.2.2 here.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "Y = iris_data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "nb_classifier.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Evaluate the classifier on the training and testing data\n",
        "train_accuracy = nb_classifier.score(X_train_scaled, Y_train)\n",
        "test_accuracy = nb_classifier.score(X_test_scaled, Y_test)\n",
        "\n",
        "# Print the training and testing accuracies\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8i2a7A0JfcH"
      },
      "source": [
        "#### 1.2.3 Using Pipelines ####\n",
        "\n",
        "In the Naive Bayes Jupyter Notebook included with your Lecture 3 slides, we used a _Pipeline_ object to simplify our code. Using that example as a guide, rewrite your code above to use _Pipeline_. Some things to note:\n",
        "\n",
        "    1. The code will not be exactly the same (it will be much simpler). For example we are not using a CountVectorizer nor a TfidfTransformer. So just follow the principle. Remember to put your StandardScaler into the Pipeline.\n",
        "    2. When doing 'fit' on your model, you should input the _original_ data, not the scaled one, since we are incorporating the StandardScaler as part of our Pipeline.\n",
        "\n",
        "**Hint: Section 1.3.2 below shows you how to create a Pipeline for SVM**\n",
        "\n",
        "Use the code cell below to enter your new version using Pipelines. Remember to print out your training and testing accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ga11mpKJfcH",
        "outputId": "bcd52440-7318-4d46-e652-bf43de156f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.95\n",
            "Testing Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.2.3 here.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "Y = iris_data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with StandardScaler and GaussianNB\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('nb', GaussianNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the pipeline on the training and testing data\n",
        "train_accuracy = pipeline.score(X_train, Y_train)\n",
        "test_accuracy = pipeline.score(X_test, Y_test)\n",
        "\n",
        "# Print the training and testing accuracies\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUOigG7XJfcH"
      },
      "source": [
        "## 1.3 Creating a Support Vector Machine Classifier ###\n",
        "\n",
        "We will now create an SVM to perform our classification. There are two major SVM classifiers provided with SciKit Learn:\n",
        "\n",
        "    1. LinearSVC: An SVM that uses a linear decision boundary to classify.\n",
        "    2. SVC: An SVM that offers a wider variety of classification boundaries: Radial Basis Function (so-called 'kernel'), sigmoid, polynomials, and of course a linear boundary.\n",
        "    \n",
        "#### 1.3.1 Creating a Linear SVM ####\n",
        "\n",
        "Using your code from 1.2.3 as a guide, create a new Pipeline to train a LinearSVC with the following parameters:\n",
        "\n",
        "    - max_iter: 100000\n",
        "    - loss: hinge\n",
        "    - penalty: l2      (Note: This is 'el-two', and not 'twelve')\n",
        "    \n",
        "Use the code cell below to implement your SVM, printing out your training and testing accuraces. Please consult the SciKit Learn documentation on what these parameters mean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDuujVS8JfcH",
        "outputId": "c692f479-d99e-41f0-94f0-0c14936fc11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9166666666666666\n",
            "Testing Accuracy: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.3.1 here.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "Y = iris_data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with StandardScaler and LinearSVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', LinearSVC(max_iter=100000, loss='hinge', penalty='l2'))\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the pipeline on the training and testing data\n",
        "train_accuracy = pipeline.score(X_train, Y_train)\n",
        "test_accuracy = pipeline.score(X_test, Y_test)\n",
        "\n",
        "# Print the training and testing accuracies\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JaTzAEJfcH"
      },
      "source": [
        "***\n",
        "_Question 7: Play around with the loss and penalty parameters. E.g. try an 'l1' penalty with hinge loss, or 'l1' penalty with squared hinge loss. Does 'l2' work with the squared hinge loss function? Record your training and testing accuracies below_\n",
        "---\n",
        "\n",
        "LinearSVC with hinge loss and L1 penalty:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', LinearSVC(max_iter=100000, loss='hinge', penalty='l1'))\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "LinearSVC with squared hinge loss and L1 penalty:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', LinearSVC(max_iter=100000, loss='squared_hinge', penalty='l1'))\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "LinearSVC with squared hinge loss and L2 penalty:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', LinearSVC(max_iter=100000, loss='squared_hinge', penalty='l2'))\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "LinearSVC with hinge loss and L1 penalty:\n",
        "\n",
        "\n",
        "Training Accuracy: 0.983\n",
        "\n",
        "\n",
        "Testing Accuracy: 0.967\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "#### 1.3.2 Autotuning Hyperparameters ####\n",
        "\n",
        "In Question 7 you have played around with some of the hyperparameters for LinearSVC and may have found that it gives you different accuracy results. Selecting the right hyperparameters is always a challenge, but thankfully SciKit Learn gives us a very useful tool called \"GridSearchCV\". In the example below we see how to tweak the 'C' parameter, which controls penalties applied to the SVM parametrs, to various values of between 1 and 10. GridSearchCV will then select the C value that gives us the best possible training accuracy:\n",
        "\n",
        "```\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C':[1,10]}\n",
        "\n",
        "svm_pipe_2 = Pipeline([('scaler', StandardScaler()),\n",
        "                    ('svm', GridSearchCV(svm.LinearSVC(max_iter = 100000), params)), ])\n",
        "svm_pipe_2.fit(X_train_1, Y_train_1)\n",
        "\n",
        "Y_train_pred_1 = svm_pipe_2.predict(X_train_1)\n",
        "Y_test_pred_1 = svm_pipe_2.predict(X_test_1)\n",
        "\n",
        "print(\"SVM Train Accuracy: %3.2f\" % np.mean(Y_train_pred_1 == Y_train_1))\n",
        "print(\"SVM Test Accuracy: %3.2f\" % np.mean(Y_test_pred_1 == Y_test_1))\n",
        "```\n",
        "\n",
        "Note that the code above will not run because it's missing several variables, including X_train_1, etc. Notice that GridSearchCV is created in the Pipeline and takes svm.LinearSVC as a parameter.\n",
        "\n",
        "The \"param\" variable is a dictionary that specifies which parameters to tune (in this case just simply 'C'), and what values to use (here \\[1, 10\\] means to use between 1 and 10). You can also specify labels instead of numeric values. E.g.:\n",
        "\n",
        "```\n",
        "params = {'kernel':('linear', 'poly')}\n",
        "```\n",
        "\n",
        "GridSearchCV will try 'linear' and 'poly', specified in the tuple after 'kernel', when tuning the SVM.\n",
        "\n",
        "Use the code cell below to create a Pipeline that uses SVC (instead of LinearSVC), and applies GridSearchCV to tune the following hyperparameters:\n",
        "\n",
        "    - C: From 1 to 10 as before\n",
        "    - kernel: 'linear', 'poly', 'rbf', 'sigmoid'\n",
        "    - decision_function_shape: 'ovr', 'ovo'\n",
        "    \n",
        "***\n",
        "_Question 8: Consult the SVC documentation and write down below what each hyperparameter means. Also what is a 'decision function shape', and what is the difference between 'ovr' and 'ovo' in our decision function shape?_\n",
        "---\n",
        "\n",
        "Hyperparameter meanings:\n",
        "\n",
        "C: Regularization parameter that controls the trade-off between achieving a low training error and a low testing error. Smaller values of C increase the regularization strength.\n",
        "\n",
        "kernel: Specifies the type of kernel to be used for the decision function. It can be 'linear', 'poly', 'rbf' (radial basis function), or 'sigmoid'.\n",
        "\n",
        "decision_function_shape: Determines the type of decision function shape to use for multi-class classification.\n",
        "\n",
        "\n",
        "'ovr' (one-vs-rest) creates a binary problem for each class versus all other classes, while 'ovo' (one-vs-one) creates a binary problem for each pair of classes.\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "Remember to print out the training and testing accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LpnDcs4UJfcI",
        "outputId": "5ddc386c-e08a-4c94-b43a-ecaa17c0a6e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9666666666666667\n",
            "Testing Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    Enter your code for part 1.3.2 here.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "Y = iris_data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "params = {\n",
        "    'svm__C': [1, 10],\n",
        "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'svm__decision_function_shape': ['ovr', 'ovo']\n",
        "}\n",
        "\n",
        "# Create a pipeline with StandardScaler and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(max_iter=100000))\n",
        "])\n",
        "\n",
        "# Apply GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(pipeline, params)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best estimator and evaluate on the training and testing data\n",
        "best_estimator = grid_search.best_estimator_\n",
        "train_accuracy = best_estimator.score(X_train, Y_train)\n",
        "test_accuracy = best_estimator.score(X_test, Y_test)\n",
        "\n",
        "# Print the training and testing accuracies\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Testing Accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_NoiyIeJfcI"
      },
      "source": [
        "### 1.4 Summary ###\n",
        "\n",
        "\n",
        "***\n",
        "_Question 9: Summarize in the table given below all the training and testing accuracies you've had in the previous section.  Give your thoughts on the performance of the various classifiers, and on using GridSearchCV to search for the right hyperparameters._\n",
        "---\n",
        "| Method            | Training Accuracy | Testing Accuracy |\n",
        "|:-----------------:|:-----------------:|:----------------:|\n",
        "| Linear Regression |  6.1922                 |   6.2429               |\n",
        "| LR (2 var)        |     5.4770              |   5.6286               |\n",
        "| LR (3 var)        |          4.9782         |    5.6952              |\n",
        "| Naive Bayes       |     0.95              |      1.0            |\n",
        "| LinearSVC         |    0.9167               |  0.9667                |\n",
        "| SVC (GridSearch)  |  0.9667                 |   1.0               |\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5ftT-1uJfcI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}