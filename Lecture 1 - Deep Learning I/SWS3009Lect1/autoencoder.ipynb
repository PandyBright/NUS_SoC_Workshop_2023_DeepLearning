{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Autoencoder\n",
    "\n",
    "In this notebook we will create a simple autoencoder that just learns a simple system represented by a linear function.\n",
    "\n",
    "We will use a simple Dense network with Stochastic Gradient Descent optimizer. \n",
    "\n",
    "## Importing the Libraries.\n",
    "\n",
    "We begin by importing our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Autoencoder\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# For generating our fake data\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some hyperparameters, including # of epochs to train, # of datapoints, as well as noise and # of testing data.\n",
    "\n",
    "Note that we have DATA_NOISE set at approximately 10% of our maximum and minimum data values, and TEST_NOISE set at 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train\n",
    "EPOCHS = 200\n",
    "\n",
    "# Number of fake datapoints to create\n",
    "                                                                                                               \n",
    "DATA_SIZE = 10000\n",
    "DATA_NOISE = 0.1\n",
    "VAL_SIZE = int(0.2 * DATA_SIZE)\n",
    "\n",
    "# Test Noise level\n",
    "TEST_NOISE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Simple Autoencoder\n",
    "\n",
    "We will now create our autoencoder. We use a fully connected dense network with 3 hidden layers with 32, 16 and 32 hidden nodes. We use ReLu activation in all hidden layers and tanh for the output layer, which allows us to create and learn data from \\[-1, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Functional API to create our autoencoder\n",
    "aeinput = Input(shape = (1, ), name = 'input')\n",
    "encoder = Dense(units = 4, activation = 'relu')(aeinput)\n",
    "encoder = Dense(units = 8, activation = 'relu')(encoder)\n",
    "encoder = Dense(units = 16, activation = 'relu')(encoder) \n",
    "decoder = Dense(units = 8, activation = 'relu')(encoder)\n",
    "decoder = Dense(units = 4, activation = 'relu')(decoder)\n",
    "aeoutput = Dense(units = 1, activation = 'tanh')(decoder)\n",
    "\n",
    "ae = Model(aeinput, aeoutput)\n",
    "ae.compile(loss = 'mean_squared_error', optimizer = 'sgd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data\n",
    "\n",
    "Let's now generate the data! We set a fixed random seed (24601.. haha), then create our random datapoints for X between 0 and 1. We then produce our Y data which goes from -1 to 1, and add some synthetic noise.\n",
    "\n",
    "Finally we call np.array to turn everything into NumPy arrays, which is needed by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set a fixed seed so that our experiments are reproducible\n",
    "random.seed(24601)\n",
    "\n",
    "# This adds white noise of -scale to scale\n",
    "def noise(scale):\n",
    "    return (2 * random.uniform(0, 1) - 1) * scale \n",
    "\n",
    "def gen_X(data_size, noise_level):\n",
    "    return [random.uniform(0, 1) + noise(noise_level) for i in range(data_size)]\n",
    "\n",
    "def gen_WrongX(data_size, noise_level):\n",
    "    return [random.uniform(0, 1.5) + noise(noise_level) for i in range(data_size)]\n",
    "\n",
    "# We create our dummy dataset and output, with 1000 numbers\n",
    "X_in = np.array(gen_X(DATA_SIZE, DATA_NOISE))\n",
    "X_test = np.array(gen_X(VAL_SIZE, DATA_NOISE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Noisy and Wrong Data\n",
    "\n",
    "We now create our very noisy data, as well as data that is produced using the wrong function ($1.5 \\times x$ instead of $2 \\times x$), so that we can evaluate how our simple autoencoder behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a noisy version of Y_out to check the anomaly detection\n",
    "X_noisy = np.array(gen_X(DATA_SIZE, TEST_NOISE))\n",
    "\n",
    "# We create an incorrect version of Y_out\n",
    "X_wrong = np.array(gen_WrongX(DATA_SIZE, DATA_NOISE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Finally let's train our autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit(x = X_in, y = X_in, batch_size = 100, \n",
    "epochs = EPOCHS, validation_data = (X_test, X_test))\n",
    "\n",
    "clean_loss = ae.evaluate(x = X_in, y = X_in)\n",
    "test_loss = ae.evaluate(x = X_test, y = X_test)\n",
    "noisy_loss = ae.evaluate(x = X_noisy, y = X_noisy)\n",
    "wrong_loss = ae.evaluate(x = X_wrong, y = X_wrong)\n",
    "\n",
    "print(\"Clean loss = %3.4f, Test loss = %3.4f Noisy loss = %3.4f, Wrong loss = %3.4f\" % \n",
    "(clean_loss, test_loss, noisy_loss, wrong_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the losses for the:\n",
    "\n",
    "    - Training data\n",
    "    - Test data\n",
    "    - Very noisy test data\n",
    "    - Incorrect data\n",
    "    \n",
    "From here we can see how the autoencoder behaves for each type of test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_loss = ae.evaluate(x = X_in, y = X_in)\n",
    "test_loss = ae.evaluate(x = X_test, y = X_test)\n",
    "noisy_loss = ae.evaluate(x = X_noisy, y = X_noisy)\n",
    "wrong_loss = ae.evaluate(x = X_wrong, y = X_wrong)\n",
    "\n",
    "print(\"\\n\\nClean loss = %3.4f, Test loss = %3.4f Noisy loss = %3.4f, Wrong loss = %3.4f\" % \n",
    "(clean_loss, test_loss, noisy_loss, wrong_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "From the results above, we can see that the test loss is very close to the training loss, but the noisy loss is much larger, and the loss from the incorrect data is a whole magnitude larger, showing that autoencoders are good at detecting abnormal data, making them good for detecting faults in systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
