{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python\n",
    "\n",
    "Linear regression in Python is available through the *sklearn* machine learning library. In this hands-on exercise we will examine the relationship between property prices and poverty levels in a particular Boston neighborhood. To begin let's import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy library provides us with some convenient ways to represent and manipulate vectors and matrices, which we will use extensively in this course. \n",
    "\n",
    "The pandas library provides very nice tools for managing datasets, while scipy is the Python scientific library with useful functions. \n",
    "\n",
    "The main star of this show is sklearn, which is the Python machine learning library, providing us with tools to create regression models using linear regression, and classification models using many models including Bayesian models and Support Vector Machines, which we will look at in this lecture.\n",
    "\n",
    "In the lab we will see how to use our own datasets, but for this lecture we will use some toy datasets that are given in sklearn. The toy datasets, amongst many others, include:\n",
    "\n",
    "- boston: Housing prices against factors like crime rate, plot ratio, poverty levels, etc, great for playing with regression models.\n",
    "- iris: Dataset on iris flowers for classification.\n",
    "- diabetes: Another regression dataset.\n",
    "- digits: A collection of 8x8 images of digits for classification.\n",
    "- wines: Data on wine types for classification.\n",
    "- breast cancer: Breast cancer dataset for regression.\n",
    "\n",
    "## Loading the Boston Dataset\n",
    "https://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "For now we will use the Boston dataset. To load it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', \n",
    " 'B', 'LSTAT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how big out dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that there's 506 pieces of data, with 13 attributes each. Let's look at what's in this dataset by printing the first 5 entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 3, suppress = True)\n",
    "d = repr(data[:5]) # Nice printable version of boston.data\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a Numpy array of 13 attributes, just as we found when we looked at the shape of the array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a very nice description of the 13 attributes that are available for regression. There is a 14th attribute which is MEDV, the median price value of properties in thousands of dollars.\n",
    "\n",
    "## Creating a Pandas Dataframe\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "To make it easier to manipulate the data, let's convert it to a Pandas dataframe, and print the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = pd.DataFrame(data)\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, but the columns don't have headings. This is set using the list bos.columns, and as it turns out the headings are available as a list in boston.feature_names. We can just set the bos.columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.columns = feature_names\n",
    "print(feature_names)\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore correlations between various columns of our data. For example, we may expect crime rate to be higher in industrial areas than in residential areas, that poverty (lstat - % of population in 'lower status') is correlated with age, or that crime rates are inversely proportional to more valuable properties, reflected by higher tax rates. We make use of correlation to test these ideas. \n",
    "\n",
    "A correlation of 1 between two factors A and B means that factor A depends perfectly on factor B, while a correlation of -1 means that factor A is perfectly negatively correlated with factor B. We can find correlation using:\n",
    "\n",
    "```\n",
    "dataframe['A'].corr(dataframe['B'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_indus = bos['CRIM'].corr(bos['INDUS'])\n",
    "pov_age = bos['LSTAT'].corr(bos['AGE'])\n",
    "crime_tax = bos['CRIM'].corr(bos['TAX'])\n",
    "\n",
    "print(\"corr crime/industry: %3.3f, corr poverty/age: %3.3f, corr crime/tax: %3.3f\" \n",
    "      % (crime_indus, pov_age, crime_tax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is some correlation between crime rate and industrial areas, and as expected poverty is strongly correlated with age. What is less expected is that there is a strong positive correlation between crime and tax rate (!!). This is something worth looking at. :) But that's not why we are here.\n",
    "\n",
    "## Finding the Relationship Between Property Prices and Poverty Levels\n",
    "\n",
    "Let's see how we can create a linear regression model to find the relationship between property prices and poverty levels. In the Boston dataset, we can find the property prices in boston.target. We first create a new column called \"price\" in the Pandas dataframe, and then check that there is actually a correlation between property prices and poverty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos['PRICE'] = target\n",
    "print(\"Correlation between property prices and poverty levels: %3.3f\"\n",
    "     % bos['PRICE'].corr(bos['LSTAT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a very sharp negative correlation between property prices and poverty levels, as we would expect. Let's start building our training data from the Pandas dataframe:\n",
    "\n",
    "### Creating the Data\n",
    "\n",
    "We will create the training data by extracting the target or \"dependent variable\" (property prices) and the data driving the target (the \"independent variable\"). We then use linear regression to find the model relating independent and dependent variables.  We begin by converting both variables to from row vectors to column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bos['LSTAT'].values.reshape(-1, 1)\n",
    "Y = bos['PRICE'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Training and Testing Data\n",
    "https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "Before we create a model we always want to split the data into training data and testing (or validation) data. This allows us to test the model's \"goodness of fit\" against data it has seen for training, and data it has never seen before ('unseen data'). This is to test for \"overfitting\", where the model memorizes the training data and has excellent result, but produces very poor results for unseen data. We want to ensure that the results for both training data and testing data do not vary too greatly.\n",
    "\n",
    "We will use the train_test_split function in sklearn to put aside 33% of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will verify that we indeed have 33% of our data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = X.shape[0]\n",
    "test_rows = X_test.shape[0]\n",
    "print(\"%%age of data used for test: %3.2f%%\" % (test_rows / all_rows * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Regression Model\n",
    "\n",
    "Excellent! Now let's build our regression model. We start by importing the LinearRegression class from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the fit method to learn from our data, and look at the coefficient and intercept for our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, Y_train)\n",
    "print(\"Coefficient: %3.4f, Intercept: %3.4f.\" % (lm.coef_, lm.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient tells us how property prices fall (since it is negative) for each percent increase in poverty, while the intercept is a base value at 0% poverty.  This gives us some interesting insight into how poverty affects property prices in Boston.\n",
    "\n",
    "### Evaluating for Over-fitting\n",
    "\n",
    "We briefly mentioned overfitting earlier in this document; a model has \"overfitted\" its training data if it can perform very well predicting the outputs for training data, yet perform very poorly on data it has never seen before.\n",
    "\n",
    "So now lets now look at how well this model fits the training data and the testing data. We will take the root-mean-square error (RMSE) given by:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{\\sum_{i=0}^{n-1}(y_{a,i} - y_{m,i})^2}{n} }\n",
    "$$\n",
    "\n",
    "Here $y_{a,i}$ is the i-th *actual* value from the data, and $y_{m, i}$ is the corresponding output from the model.\n",
    "\n",
    "On its own the RMSE is fairly useless, but we can use it to compare the prediction error of the model when it is using training data it has seen, and testing data it has never seen.\n",
    "\n",
    "We call lm.predict(X_train) and lm.predict(X_test) to produce the predicted property prices using the training and testing data respectively, then call the mean_squared_error function from the sklearn.metrics package and square-root the result. Recall that our target values are in Y_train and Y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Y_pred_train = lm.predict(X_train)\n",
    "Y_pred_test = lm.predict(X_test)\n",
    "train_mse = np.sqrt(metrics.mean_squared_error(Y_train, Y_pred_train))\n",
    "test_mse = np.sqrt(metrics.mean_squared_error(Y_test, Y_pred_test))\n",
    "print(\"RMSE for training data: %3.4f. RMSE for testing data: %3.4f.\" % (train_mse, test_mse))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On its own the RMSE is fairly close. This means that the model performs about as well on data it has never seen, as on training data it has seen. So we are confident that the model is sufficiently \"general\" and has not memorized the training data to the detriment of unseen data.\n",
    "\n",
    "Note however that the RMSE does not give us a really good idea of how accurate our model is in absolute terms, just in relative terms. We can still use this however to compare against other models we create.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Our Model\n",
    "\n",
    "We can now make some predictions on our model. We look at property prices if the poverty level was 10%, versus prices when the property level is 25%. We note that the prices are in units of $1,000 and multiply accordingly.\n",
    "\n",
    "Note that lm.predict requires a 2D numpy array, which we create using np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price1 = lm.predict(np.array([[10.0]]))\n",
    "price2 = lm.predict(np.array([[25.0]]))\n",
    "\n",
    "print(\"Price at 10%% poverty level is $%3.2f. Price at 25%% poverty level is $%3.2f\"\n",
    "      %(price1 * 1000, price2 * 1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that at a 10% poverty level the mean pricing for housing is \\\\$25,129.90, which drops to \\\\$10,868.97 when the poverty level rises to 25%. We can manually verify that this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = -0.9507 * 10 + 34.6372\n",
    "p2 = -0.9507 * 25 + 34.6372\n",
    "\n",
    "print(\"Price at 10%% poverty level is $%3.2f. Price at 25%% poverty level is $%3.2f\"\n",
    "      %(p1 * 1000, p2 * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an almost perfect match between the two models. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
