{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification in Python\n",
    "\n",
    "In this notebook we will look at how to create a Naive Bayes classifier in Python. As before we will use a toy dataset from Scikit-Learn, in this case the 20 Newsgroups Dataset.\n",
    "\n",
    "## 20-Newsgroups\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "\n",
    "In the days before the web, bored scientists and engineers would discuss matters of global concern over a text-based bulletin-board system (BBS) called USENET. \"News\" articles were distributed over USENET using a protocol called \"Network News Transfer Protocol\" or NNTP, and a text-based reader like \"tin\" would be used to read and write/respond to these articles.\n",
    "\n",
    "News articles in USENET are divided into \"Newsgroups\" with names like alt.tv.simpsons which discusses the Simpsons TV show, and comp.programming which discusses - surprisingly - programming.\n",
    "\n",
    "In this section we will use the 20 Newsgroups dataset, which consists of 20,000 news articles gathered across 20 newsgroups. To begin, we import the various packages we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the dataset. As it is quite large we will only load the training data, and leave the testing data for later. We set 'shuffle' to be True so that the data is shuffled and different each time.\n",
    "\n",
    "We then print the names of each newsgroup, given in \"target_names\". Note that it will take a few minutes to run, and the \"In \\[6\\]\" marker on the left will show as \"In \\[\\*\\]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset = 'train', shuffle = True)\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few minutes you should see the names of the 20 newsgroups that are captured in the dataset. Let's look at how long this training dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_LEN = 20000 # Total number of articles\n",
    "train_len = len(twenty_train.data)\n",
    "print(\"%%age of dataset used for training: %3.4f\" % (train_len / FULL_LEN * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 57% has been used for training, and 43% for testing. Now let's start classifying!\n",
    "\n",
    "## Using the SK-Learn Naive Bayes Classifiers\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\n",
    "\n",
    "In the lecture we saw that there are four types of Naive Bayes classifiers (+1 classifier that is a variant of Multinomial NB):\n",
    "\n",
    "- Gaussian (GaussianNB), used for continuous features like temperature, height, etc.\n",
    "\n",
    "- Multinomial (MultinomialNB), if each feature is some kind of count. \n",
    "\n",
    "- \"Complement Naive Baye\" (ComplementNB) that is like MultinomialNB, but does special calculations to overcome the effects of imbalanced datasets. Since the 20 Newsgroups has about 1000 articles per newsgroup (i.e. it is balanced), we will not use this.\n",
    "\n",
    "- Bernoulli (BernoulliNB), if each feature is a binary value.\n",
    "\n",
    "Since our dataset consists of word counts, we will use the MultinomialNB model. There are several things we need to do first.\n",
    "\n",
    "### Creating a Bag of Words\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "We begin by creating a \"bag of words\", which is essentially a count of each word as they occur in each article. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_counts = cv.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an array of articles, with each article containing a vector of counts, and index pointers telling us where each article starts and ends in the bag of words, etc. We can explore some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of articles: %d\" % X_train_counts.shape[0])\n",
    "print(\"Count for words in the first article: \", repr(X_train_counts[0].data))\n",
    "print(\"Indexes of words in the first article: \", repr(X_train_counts[0].indices))\n",
    "\n",
    "print(\"The first 5 words in the first article:\\n\")\n",
    "\n",
    "for ind in X_train_counts[0].indices[:5]:\n",
    "    print(cv.get_feature_names()[ind],\" \", end=\"\")\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let's create our classifier.\n",
    "\n",
    "### Building the 20 Newsgroups Classifier\n",
    "\n",
    "Since our data consists of counts of each word, we will use a Multinomial Bayesian Classifier, as discussed in the lecture. We will import the MultinomialNB class, then call the \"fit\" method to train the model. We will also load up the test data from the dataset.\n",
    "\n",
    "Note that when we create the word count for the documents in the test data, we use transform instead of fit_transform. This is because fit_transform will create a new dictionary. All we want to do is to convert the documents in the test set into a bag of words; we do not want to learn a new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train using our training data\n",
    "clf.fit(X_train_counts, twenty_train.target)\n",
    "\n",
    "# Load up the test data\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle = True)\n",
    "\n",
    "# We call transform to turn twenty_test into a BOW. We DO NOT call\n",
    "# fit_transform, which also learns the words.\n",
    "X_test_counts = cv.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model trained, we can now evaluate its performance. We will create a vector of predicted categories and compare them against the \"ground truths\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test_counts)\n",
    "\n",
    "# predicted == twenty_test.target will produce a vector with \"1\" where the labels in \n",
    "# predicted match those in the target, and a \"0\" otherwise. We then call np.mean that\n",
    "# sums up this vector and divides by the # of elements, effectively giving an accuracy rate.\n",
    "perf = np.mean(predicted == twenty_test.target)\n",
    "print(\"Our 20 Newsgroup's raw-count classifer correctly classified %3.4f%% of the articles.\" \n",
    "      % (perf * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is 77.28%! Not bad, but not great either. Let's using tf.idf instead of raw word counts, and see what happens.\n",
    "\n",
    "### Using tf.idf For Classification\n",
    "\n",
    "As mentioned in the lecture raw word counts have two problems:\n",
    "\n",
    "1. A bias towards long documents and terms that occur frequently across documents (and are thus very bad 'discriminators' - attributes that help us differentiate between document classes)\n",
    "\n",
    "2. Zero count terms.\n",
    "\n",
    "To fix this we use tf.idf, which if you recall, is defined as:\n",
    "\n",
    "$$\n",
    "x_i = log(tf_{ik} + 1) log(\\frac{D}{d_{tf}})\n",
    "$$\n",
    "\n",
    "The $log(tf_{ik} + 1)$ part eliminates zero count terms by adding 1 (and taking a log so that the numbers don't become too huge), while the $log(\\frac{D}{d_{tf}})$ part punishes terms that occur too frequently across many documents.\n",
    "\n",
    "To get the tf.idf BOW, we use TfidfTransformer and use the raw count BOW derived in the previous part. The rest of the code is fairly straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "tfidf_clf = MultinomialNB()\n",
    "tfidf_clf.fit(X_train_tfidf, twenty_train.target)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predicted_tfidf = tfidf_clf.predict(X_test_tfidf)\n",
    "perf_tfidf = np.mean(predicted_tfidf == twenty_test.target)\n",
    "\n",
    "print(\"Our 20 Newsgroup's tf-idf classifer correctly classified %3.4f%% of the articles.\" \n",
    "      % (perf_tfidf * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have a slight improvement in performance of over 0.11% \n",
    "\n",
    "### Using Stop Words\n",
    "\n",
    "The CountVectorizer allows us to ignore stop-words. Stop-words are words that occur very frequently, like \"a\", \"the\", etc, that they are meaningless for classification. You can find an example stop-word list at https://countwordsfree.com/stopwords.\n",
    "\n",
    "Since the stop-words are ignored at the CountVectorizer, this means that we need to recreate all our counts again. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_count_vect = CountVectorizer(stop_words = 'english')\n",
    "X_train_counts = sw_count_vect.fit_transform(twenty_train.data)\n",
    "X_test_counts = sw_count_vect.transform(twenty_test.data)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "tfidf_clf = MultinomialNB()\n",
    "tfidf_clf.fit(X_train_tfidf, twenty_train.target)\n",
    "predicted_tfidf_sw = tfidf_clf.predict(X_test_tfidf)\n",
    "\n",
    "perf_tfidf_sw = np.mean(predicted_tfidf_sw == twenty_test.target)\n",
    "\n",
    "print(\"Our 20 Newsgroup's tf-idf with stop-words classifer correctly classified %3.4f%% of the articles.\" \n",
    "      % (perf_tfidf_sw * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a substantial improvement of 4%!\n",
    "\n",
    "## Using Pipelines\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "The current workflow for creating classifiers and regressors is tedious; fortunately Scikit-Learn provides a very useful structure called a Pipeline that lets us specify what objects to use to process the inputs to produce the outputs. The code below shows how to use a Pipeline. Essentially the Pipeline takes a list of tuples that contain:\n",
    "\n",
    "- An string identifier that you can use later on to access a particular object in the pipeline.\n",
    "\n",
    "- The object itself that is used to process the data.\n",
    "\n",
    "Let's look at how to create our tf.idf classifier with stop-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = 'english')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB()), ])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_pipeline = text_clf.predict(twenty_test.data)\n",
    "perf_pipeline = np.mean(predicted_pipeline == twenty_test.target)\n",
    "print(\"Our 20 Newsgroup's pipeline classifer correctly classified %3.4f%% of the articles.\" \n",
    "      % (perf_pipeline * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we have exactly the same results as before. Notice how much easier and more intuitive this is; we just specify the steps in our processing - CountVectorizer which produces the bag of words, tfidfTransformer that turns the raw frequencies into tf.idf scores, and finally MultinomialNB which does the classifications.\n",
    "\n",
    "We provide labels like 'vect', 'tfidf' and 'clf' that lets us access these individual objects. For example we could do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_clf['clf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which as we can see returns us the MultinomialNB we put into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
