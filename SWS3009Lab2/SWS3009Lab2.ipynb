{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b8d876",
   "metadata": {},
   "source": [
    "# Lab 2. Building and Using Transformers\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In the previous lab we looked at how to build an LSTM to generate text. In this lab we will look at how to do the same thing with transformers.\n",
    "\n",
    "In the lecture we have already seen how transformers are built in Keras. In this lab rather than building our transformers from scratch, we will use pre-made models on Hugging Face. In the first part we will build a story generator from a Hugging Face template, and in the second part we will look at how to build the same story generator using a Generative Pretrained Transformer (GPT).\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "### SUBMISSION DEADLINE: Saturday 8 July 2023, 2359 hours (11.59 pm)\n",
    "\n",
    "Please submit all your answers in the <b>Lab2AnsBk.docx</b> answer book provided. Ensure that the names of <b>both</b> team members is in the answer book. However you should only submit <b>one copy</b> of the lab report on Canvas.\n",
    "\n",
    "<b>Please PDF your answer book and submit only ONE pdf per team</b>\n",
    "\n",
    "## 3. Building our Transformer Based Story Generator\n",
    "\n",
    "We will now proceed to build our story generator. As before we begin by loading our text corpus (we are again using Sherlock Holmes). Unlike LSTMs however we can simply present an entire chunk of text to the transformer. However there is an added complication in that the chunks must be of <b>fixed length</b>.\n",
    "\n",
    "### 3.1 Loading our Dataset\n",
    "\n",
    "As before we will use glob to scan the training and testing directory, then loading the files into the dataset. We filter out the sentences that have fewer than 5 words, then convert all the text to lowercase. This part exactly the same as in Lab 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import glob\n",
    "traindir=\"sherlock/Train/\"\n",
    "testdir=\"sherlock/Test/\"\n",
    "trainlist = [file for file in glob.glob(traindir+\"*.txt\")]\n",
    "testlist = [file for file in glob.glob(testdir+\"*.txt\")]\n",
    "\n",
    "print(trainlist)\n",
    "# Our training files\n",
    "data_files ={\"train\":trainlist,\n",
    "           \"test\":testlist}\n",
    "\n",
    "# Now load the dataset\n",
    "dataset = load_dataset(\"text\", data_files=data_files)\n",
    "\n",
    "# Discard statements with fewer than 5 words\n",
    "min_len = 5\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) >= min_len)\n",
    "\n",
    "# Turn all our text to lowercase\n",
    "def preprocess(example):\n",
    "    return {\"text\":example[\"text\"].lower()}\n",
    "\n",
    "dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876b80a",
   "metadata": {},
   "source": [
    "## 3.2 Training the Tokenizer\n",
    "\n",
    "We now tokenize the dataset. This means mapping all the sentences to vectors of integers. However unlike in Lab 1 we will create a custom tokenizer by adapting the gpt2 tokenizer we used in Lab 1.\n",
    "\n",
    "(If you want to train a tokenizer and language model <b><i>from scratch</i></b>, which is very useful for new languages, please see here: https://huggingface.co/blog/how-to-train)\n",
    "\n",
    "We start by loading the gpt2 tokenizer from Hugging Face. Note that transformers require our sentences to be of <b>fixed length</b>, unlike LSMs in Lab 1 that just require the last <i>lookback</i> tokens.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Explain why transformers train using entire sentences instead of short \"look back\" sentences like LSTM.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Explain why the sentences used to train the transformers must be of fixed length.\n",
    "\n",
    "<b>Fill the answers to both questions inside the provided answer book</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e107887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "# Load the model.\n",
    "model_name = \"gpt2\"\n",
    "root_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True,\n",
    "                                         max_length=max_length)\n",
    "#Specify the padding token\n",
    "root_tokenizer.pad_token = root_tokenizer.eos_token\n",
    "\n",
    "vocab_size = len(root_tokenizer)\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5cd61",
   "metadata": {},
   "source": [
    "We can now start training the tokenizer using our dataset. To do so we create a Python generator by yielding a batch of <i>steps</i> sentences at a time. We do this because the dataset is too large to be completely loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Python generator to return steps sentences at a time\n",
    "# Notice that we use '()' instead of '[]' for our list. This\n",
    "# yields a generator.\n",
    "\n",
    "def return_training_corpus(dataset, steps):\n",
    "    train = dataset[\"train\"]\n",
    "    return (train[i:i+steps][\"text\"] for i in range(0, len(train), steps))\n",
    "\n",
    "\n",
    "# Create the generator that returns 1000 sentences at a time.\n",
    "# We have about 15,000 sentences in the Sherlock dataset\n",
    "\n",
    "gen = return_training_corpus(dataset, 1000)\n",
    "\n",
    "# Now start training the tokenizer\n",
    "tokenizer = root_tokenizer.train_new_from_iterator(gen, vocab_size, \n",
    "                                                   length = len(dataset[\"train\"]))\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Vocab size of our new tokenizer: \", vocab_size)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"sherlock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0d5ec",
   "metadata": {},
   "source": [
    "### 3.3 Training a Transformer From Scratch\n",
    "\n",
    "We will now start creating and training a transformer from scratch. In the lecture we have seen how to build a transformer using Keras. It is unproductive to do this again, so we will use pre-configured (but untrained) transformers from Hugging Face. \n",
    "\n",
    "There are several things we need to do:\n",
    "\n",
    "    1. Tokenize the entire dataset, creating fixed-length sentences\n",
    "    2. Load up a pre-configured transformer. We are using the pretrained GPT2 language model.\n",
    "    3. Train the pre-configured transformer and save it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire corpus by using map\n",
    "def tokenize(example):\n",
    "    outputs = tokenizer(example[\"text\"], padding=True, truncation=True,\n",
    "                        max_length=max_length, return_overflowing_tokens=True)\n",
    "    \n",
    "    ret_tokens=[]\n",
    "    \n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        ret_tokens.append(input_ids)\n",
    "\n",
    "    # Map requires a dictionary of tokens to be returned. The token entries\n",
    "    # must be called \"input_ids\"\n",
    "    return {\"input_ids\":ret_tokens}\n",
    "\n",
    "# We must set batched = True so that the tokenizer knows how many characters to pad to.\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns = dataset[\"train\"].column_names,\n",
    "                               batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0802be4",
   "metadata": {},
   "source": [
    "We can now print the lengths of the first 10 sentences to show you that they've all been padded/truncated to the same length, which is the length of the longest statement seen (or at most 30 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71275ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(tokenized_dataset[\"train\"][\"input_ids\"][:10]):\n",
    "    print(\"Length of sentence \", i, \": \", len(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac36c38",
   "metadata": {},
   "source": [
    "Great! Now let's build our transformer. We will create it from an existing GPT-2 transformer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the configuration and transformer\n",
    "from transformers import AutoConfig, TFGPT2LMHeadModel\n",
    "\n",
    "# Load configuration from existing GPT2 network. Set length of sentences,\n",
    "# start of sentence and end of sentence tokens\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, \n",
    "                                    vocab_size = len(tokenizer), \n",
    "                                    n_ctx = max_length,\n",
    "                                   bos_token_id = tokenizer.bos_token_id,\n",
    "                                   eos_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "# Create the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2b400",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "\n",
    "Using the Hugging Face website or otherwise, explain the parameters in AutoConfig.from_pretrained that we have used.\n",
    "\n",
    "Now we are going to start training the new model. Before this we need to create a data collator that will batch the inputs for training. We then convert the dataset into a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mlm = Masked Language Model, where we masked random words and let\n",
    "# the language model infer what it is. We are not doing that here so\n",
    "# we set mlm to False.\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the TensorFlow datasets\n",
    "tf_train_set = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "tf_test_set = tokenized_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a707d",
   "metadata": {},
   "source": [
    "Now we begin training the Transformer! We will use an Adam optimizer to train for 5 epochs or do early termination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba08758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import AdamWeightDecay\n",
    "import os\n",
    "\n",
    "# tsherlock is the transformer version of the sherlock story generator\n",
    "filename=\"./tsherlock.h5\"\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0.01, patience=2)\n",
    "\n",
    "# Compile the model \n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# If the weights file exists, load it\n",
    "if os.path.exists(filename):\n",
    "    # Call model build to initialize the model\n",
    "    # variables, so that we can call load_weights\n",
    "    model.build(input_shape=(None, ))\n",
    "    model.load_weights(filename)\n",
    "    \n",
    "# Train the model. This will take a REALLY long time.\n",
    "epochs=5\n",
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs,\n",
    "         callbacks=[earlystop])\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ada84",
   "metadata": {},
   "source": [
    "### 3.4 Generating Stories\n",
    "\n",
    "Generating stories using Hugging Face is considerably easier; we can just define a pipeline with our model and tokenizer, and tell it how many words to generate and whether or not to do sampling. \n",
    "\n",
    "We are going to use a text generation pipeline. You can get a complete list of available pipelines here: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "Let's try this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe=pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Number of words to generate\n",
    "num_words = 100\n",
    "\n",
    "# Our seed sentence\n",
    "seed=\"Elementary my dear Watson, \"\n",
    "text = pipe(seed, max_length=num_words, do_sample=True, no_repeat_ngram_size=2)[0]\n",
    "\n",
    "print(\"Generated text: \\n\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e71825",
   "metadata": {},
   "source": [
    "### 3.5 Fine-tuning a Pretrained Transformer\n",
    "\n",
    "We will now fine-tune a pretrained transformer and compare the speed and results. We begin by bringing in the pretrained model using TFAutoModelForCausalLM, then use fit as usual to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65aef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "# The from_pt parameter is to tell the model to convert the weights from PyTorch\n",
    "# format. We will use the same optimizer as before but set a new checkpoint\n",
    "# with a new filename\n",
    "\n",
    "pretrained_filename = \"ptsherlock.h5\"\n",
    "\n",
    "pretrained_model = TFAutoModelForCausalLM.from_pretrained(model_name, from_pt=True)\n",
    "pretrained_model.compile(optimizer=optimizer)\n",
    "\n",
    "# If the weights file exists, load it\n",
    "if os.path.exists(pretrained_filename):\n",
    "    pretrained_model.build(input_shape=(None,))\n",
    "    pretrained_model.load_weights(pretrained_filename)\n",
    "\n",
    "pretrained_model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs,\n",
    "         callbacks=[earlystop])\n",
    "\n",
    "pretrained_model.save_weights(pretrained_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918225cf",
   "metadata": {},
   "source": [
    "As before we start generating our stories so we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_pipe = pipeline(\"text-generation\", model=pretrained_model, tokenizer=tokenizer)\n",
    "pretrained_text = pretrained_pipe(seed, max_length=num_words, do_sample=True, \n",
    "                               no_repeat_ngram_size=2)[0]\n",
    "\n",
    "print(\"Generated Text from new Transformer: \\n\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nPretrained Generated Text: \\n\")\n",
    "print(pretrained_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df53db9",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Compare the texts generated from the transformer that was trained from scratch versus the transformer that used the pretrained GPT2 weights. Do you see a difference in quality, e.g. fewer \"non-English\" words?\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "This lab is a follow-up to Lab 1, and here we use transformers to generate stories instead of LSTMs. We started by training a transformer from scratch, and then proceeded to fine-tune a pretrained model.\n",
    "\n",
    "Hugging Face has many, many models that you can work with, and this lab should serve as an introduction. You are encouraged to explore the other models, and how to use train them and use them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
